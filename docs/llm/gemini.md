# LLM Provider Implementation Guide

This document describes the implementation of all LLM providers (Google Gemini, OpenAI, and Anthropic) for the AI voice agent system.

## Overview

The system implements three standardized LLM providers that all conform to the `BaseLLMProvider` interface:

- **GeminiProvider**: Google Gemini integration with latest models
- **OpenAIProvider**: OpenAI GPT models integration  
- **AnthropicProvider**: Anthropic Claude models integration

All providers support both single response generation and streaming responses, making them suitable for real-time voice conversations.

## Universal Features

All providers implement:

- ✅ **Single Response Generation**: Generate complete responses
- ✅ **Streaming Responses**: Stream response tokens for real-time applications
- ✅ **Configuration Validation**: Test and validate API configuration
- ✅ **Error Handling**: Robust error handling with detailed logging
- ✅ **Token Usage Tracking**: Track token usage when available
- ✅ **Multi-turn Conversations**: Support for conversation history and context
- ✅ **Latest Model Support**: Up-to-date model lists with newest versions

## Installation

All required dependencies are included in `requirements.txt`:

```bash
pip install google-generativeai>=0.3.2 openai>=1.0.0 anthropic>=0.3.0
```

## Configuration

### Environment Variables

Set API keys for the providers you plan to use:

```bash
export GEMINI_API_KEY="your-gemini-api-key"
export OPENAI_API_KEY="your-openai-api-key"  
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### Available Models

#### Gemini Models (Latest)

- **gemini-1.5-flash** (default) - Fast, efficient for most tasks
- **gemini-1.5-pro** - Advanced reasoning and complex tasks
- **gemini-1.5-flash-8b** - Lightweight version
- **gemini-1.0-pro** - Legacy stable version

#### OpenAI Models (Latest)

- **gpt-4o-mini** (default) - Fast, cost-effective
- **gpt-4o** - Most capable model
- **gpt-4-turbo** - High performance with large context
- **gpt-3.5-turbo** - Reliable for most use cases

#### Anthropic Models (Latest)

- **claude-3-5-sonnet-20241022** (default) - Latest and most capable
- **claude-3-5-haiku-20241022** - Fast and efficient
- **claude-3-opus-20240229** - Most powerful for complex tasks
- **claude-3-sonnet-20240229** - Balanced performance

## Provider-Specific Configuration
```

## Usage Examples

### Basic Usage

```python
from services.llm.providers.gemini import GeminiProvider
from models.external.llm.request import LLMRequest, LLMMessage

# Initialize provider
provider = GeminiProvider()

# Create request
messages = [
    LLMMessage(role="system", content="You are a helpful assistant."),
    LLMMessage(role="user", content="Hello!")
]

request = LLMRequest(
    messages=messages,
    model="gemini-pro",
    temperature=0.7,
    max_tokens=150
)

# Generate response
response = await provider.generate_response(request)
print(response.get_content())
```

### Streaming Response

```python
# Create streaming request
request = LLMRequest(
    messages=messages,
    model="gemini-pro",
    temperature=0.7,
    max_tokens=150,
    stream=True
)

# Stream response
async for chunk in provider.stream_response(request):
    print(chunk, end="", flush=True)
```

### Configuration Validation

```python
# Validate configuration
is_valid = await provider.validate_config()
if is_valid:
    print("Provider is ready to use!")
else:
    print("Configuration issues detected.")
```

## Integration with LLM Orchestrator

The Gemini provider is automatically registered with the LLM orchestrator:

```python
from services.llm.orchestrator import LLMOrchestrator

orchestrator = LLMOrchestrator()

# Use Gemini through orchestrator
response = await orchestrator.generate_response(
    messages=[{"role": "user", "content": "Hello!"}],
    provider="gemini",
    model="gemini-pro"
)
```

## Message Format Conversion

The provider automatically converts the standard message format to Gemini's expected format:

### Input Format (Standard)
```python
[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there!"},
    {"role": "user", "content": "How are you?"}
]
```

### Converted Format (Gemini)
```text
[System]: You are a helpful assistant.
[User]: Hello!
[Assistant]: Hi there!
[User]: How are you?
```

## Safety Settings

The provider includes configurable safety settings to manage the content generated by the Gemini model. You can adjust these settings based on your business requirements to filter out unwanted or unsafe content.

### Configuration

Safety settings can be configured in the provider configuration:

```python
config = {
    ...
    "safety_settings": {
        "enable": True,                   # Enable safety settings (default: True)
        "level": "high",                 # Safety level: low, medium, high (default: high)
        "custom_warnings": ["violence", "hate_speech"]  # Custom warning categories
    }
}
```

### Usage

The safety settings are automatically applied during response generation. You can also manually check content safety:

```python
# Check content safety
is_safe = await provider.check_content_safety(response.get_content())
if not is_safe:
    print("Response contains unsafe content!")
```

## Error Handling

The provider includes robust error handling to manage issues that may occur during API calls or response processing. Errors are logged with detailed information to assist in troubleshooting.

### Error Types

- **ConfigurationError**: Issues with provider configuration
- **AuthenticationError**: Invalid or missing API key
- **QuotaError**: Exceeded API usage quota
- **RateLimitError**: Too many requests in a short period
- **InvalidRequestError**: Malformed or invalid request parameters
- **GeminiError**: General error from the Gemini API

### Usage

Errors are automatically caught and logged by the provider. You can also implement custom error handling:

```python
try:
    response = await provider.generate_response(request)
except Exception as e:
    print(f"An error occurred: {e}")
    # Handle specific error types if needed
    if isinstance(e, AuthenticationError):
        print("Invalid API key. Please check your configuration.")
```

## Troubleshooting

If you encounter issues while using the Gemini provider, here are some common problems and solutions:

- **Invalid API Key**: Ensure that the `GEMINI_API_KEY` environment variable is set and the API key is valid.
- **Quota Exceeded**: Check your Google Cloud Console for API usage and quota limits.
- **Rate Limiting**: If you receive rate limit errors, try reducing the frequency of requests or implementing exponential backoff.
- **Malformed Requests**: Ensure that the request parameters are correctly set and match the expected format.
- **Timeouts**: If requests are timing out, try increasing the timeout duration or check your network connection.

For further assistance, consult the [Google Gemini API documentation](https://cloud.google.com/gemini/docs) or contact Google Cloud support.
